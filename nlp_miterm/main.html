<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NLP Midterm Cheat Sheet — Ch.1–4</title>
  <style>
    :root{ --bg:#0b0f19; --card:#121826; --muted:#9fb0c5; --text:#e6eef8; --accent:#7dd3fc; --accent2:#a78bfa; --ok:#34d399; --warn:#fbbf24; }
    *{box-sizing:border-box}
    html,body{margin:0;padding:0;background:var(--bg);color:var(--text);font:16px/1.6 system-ui, -apple-system, Segoe UI, Roboto, Noto Sans KR, Apple SD Gothic Neo, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji"}
    a{color:var(--accent);text-decoration:none}
    .wrap{max-width:1100px;margin:32px auto;padding:0 20px}
    header{position:sticky;top:0;backdrop-filter:saturate(140%) blur(6px);background:linear-gradient(180deg,rgba(11,15,25,.9),rgba(11,15,25,.4));z-index:10;border-bottom:1px solid rgba(255,255,255,.06)}
    header .bar{max-width:1100px;margin:0 auto;padding:12px 20px;display:flex;align-items:center;gap:12px}
    .badge{padding:2px 8px;border-radius:999px;background:rgba(125,211,252,.15);color:var(--accent);font-size:12px}
    h1{font-size:28px;margin:16px 0 8px}
    h2{font-size:22px;margin:24px 0 12px;color:#d7e3f5}
    h3{font-size:18px;margin:18px 0 8px;color:#d7e3f5}
    .card{background:var(--card);border:1px solid rgba(255,255,255,.06);border-radius:16px;padding:18px;margin:16px 0;box-shadow:0 6px 30px rgba(0,0,0,.25)}
    .grid{display:grid;gap:16px}
    .g-2{grid-template-columns:repeat(2,minmax(0,1fr))}
    .g-3{grid-template-columns:repeat(3,minmax(0,1fr))}
    @media (max-width:900px){.g-2,.g-3{grid-template-columns:1fr}}
    .muted{color:var(--muted)}
    .kicker{color:var(--accent2);letter-spacing:.04em;font-weight:600}
    .formula{padding:12px 14px;border-radius:12px;background:rgba(167,139,250,.08);border:1px solid rgba(167,139,250,.25);overflow:auto}
    .chip{display:inline-block;padding:4px 8px;border-radius:999px;background:rgba(255,255,255,.06);border:1px solid rgba(255,255,255,.08);margin:2px 6px 2px 0}
    .tip{border-left:4px solid var(--ok);padding-left:12px}
    .warn{border-left:4px solid var(--warn);padding-left:12px}
    .toc{display:flex;flex-wrap:wrap;gap:10px}
    .toc a{padding:6px 10px;border:1px solid rgba(255,255,255,.08);border-radius:10px;background:rgba(255,255,255,.04)}
    code,kbd{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;background:rgba(255,255,255,.06);border:1px solid rgba(255,255,255,.08);padding:1px 5px;border-radius:6px}
    footer{opacity:.8;margin:28px 0 40px}
  </style>
  <script>function toggleAll(open){document.querySelectorAll('details').forEach(d=>d.open=open)}</script>
  <script>
    window.MathJax = { tex: { inlineMath: [['$','$'], ['\\(','\\)']] } };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <header>
    <div class="bar">
      <span class="badge">NLP Midterm</span>
      <strong>Cheat Sheet (Ch.1–4)</strong>
      <span class="muted" style="margin-left:auto">Print-friendly HTML · 수식은 MathJax</span>
    </div>
  </header>

  <div class="wrap">

    <div class="card">
      <h1>Table of Contents</h1>
      <div class="toc">
        <a href="#ch1">Ch.1 Introduction</a>
        <a href="#ch2">Ch.2 Linear Text Classification</a>
        <a href="#ch3">Ch.3 Nonlinear Classification</a>
        <a href="#ch4">Ch.4 Sequence Labeling (HMM/CRF)</a>
        <a href="#ref">Quick Reference (암기 포인트)</a>
      </div>
    </div>

    <!-- Ch.1 -->
    <section id="ch1" class="card">
      <div class="kicker">Chapter 1</div>
      <h2>Introduction to NLP</h2>
      <div class="grid g-2">
        <div>
          <h3>정의 & 이웃 분야</h3>
          <ul>
            <li><strong>NLP</strong>: “인간 언어를 컴퓨터가 다루게 하는 방법들의 집합”.</li>
            <li>동의어로 <em>Computational Linguistics</em> 자주 사용.</li>
            <li>밀접: ML, AI, CS, 언어학, 음성처리, 윤리/공정성.</li>
          </ul>
          <h3>역사 요약</h3>
          <p><span class="chip">Symbolic(규칙)</span><span class="chip">Statistical(확률)</span><span class="chip">Neural(분산표현·딥)</span></p>
          <p class="muted">예: ELIZA → IBM alignment/LM → word2vec/BERT</p>
        </div>
        <div>
          <h3>형식문법: Chomsky Hierarchy</h3>
          <p>정규(Type-3) ⟶ 문맥자유(Type-2) ⟶ 문맥의존(Type-1) ⟶ 재귀적 열거(Type-0)</p>
          <div class="tip">시험 팁: 난이도/표현력 ↑ 순서 암기 (3→2→1→0)</div>
        </div>
      </div>
    </section>

    <!-- Ch.2 -->
    <section id="ch2" class="card">
      <div class="kicker">Chapter 2</div>
      <h2>Linear Text Classification</h2>
      <div class="grid g-2">
        <div>
          <h3>문제 정의 & 워크플로</h3>
          <ul>
            <li>문서 $x$ → 레이블 $y \in Y$ 예측 (스팸, 감정 등)</li>
            <li>파이프라인: 수집→토크나이즈→학습→평가 (Train/Val 70–90%, Test 10–30%)</li>
          </ul>
          <h3>표현: Bag-of-Words</h3>
          <p>단어 카운트 벡터 $x\in\mathbb{R}^{|V|}$, 선형 스코어 $\psi(x,y)=\theta\cdot f(x,y)$</p>
          <div class="formula">$\psi(x,y)=\sum_j \theta_j f_j(x,y)$</div>
        </div>
        <div>
          <h3>주요 모델</h3>
          <details open>
            <summary><strong>Naïve Bayes (생성적)</strong></summary>
            <div class="formula">\(\hat{\theta}=\arg\max_\theta \sum_i \log p(x^{(i)},y^{(i)};\theta)\)</div>
            <p class="muted">로그우도 최적화 (언더플로 방지)</p>
          </details>
          <details>
            <summary><strong>Perceptron (판별적)</strong></summary>
            <div class="formula">\(\hat{y}=\mathrm{sign}(\theta\cdot x + b)\)</div>
            <p class="muted">선형분리 불가(예: XOR) 데이터에서 수렴 X → <em>averaged</em> 기법</p>
          </details>
          <details>
            <summary><strong>Logistic Regression (소프트맥스)</strong></summary>
            <div class="formula">\(p(y\mid x;\theta)=\dfrac{\exp(\theta\cdot f(x,y))}{\sum_{y'}\exp(\theta\cdot f(x,y'))}\)</div>
            <div class="formula">\(\ell_{logreg}=-\,\theta\cdot f(x,y)+\log\sum_{y'}\exp(\theta\cdot f(x,y'))\)</div>
          </details>
        </div>
      </div>
      <div class="grid g-3">
        <div class="card" style="background:rgba(52,211,153,.08)">
          <h3>지표(이진)</h3>
          <div class="formula">$\text{Precision}=\tfrac{tp}{tp+fp}$ · $\text{Recall}=\tfrac{tp}{tp+fn}$ · $F_1=\tfrac{2PR}{P+R}$</div>
          <p class="muted">임계값(.5)과 클래스 불균형에 민감</p>
        </div>
        <div class="card">
          <h3>오류 주도 vs 확률적 손실</h3>
          <ul>
            <li>Perceptron: mistake-driven 업데이트</li>
            <li>LogReg: 확률적 손실(크로스엔트로피) + SGD</li>
          </ul>
        </div>
        <div class="card">
          <h3>실전 체크리스트</h3>
          <ul>
            <li>Train/Test 동일 토크나이저</li>
            <li>특징 함수 $f(x,y)$와 가중치 분리해서 사고</li>
            <li>정규화/불용어/어휘사전 정책 기록</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- Ch.3 -->
    <section id="ch3" class="card">
      <div class="kicker">Chapter 3</div>
      <h2>Nonlinear Classification</h2>
      <div class="grid g-2">
        <div>
          <h3>왜 비선형인가?</h3>
          <p>언어 데이터는 상호작용(문맥·구문)이 강해 <em>선형 결정경계</em>로 분리되지 않는 경우가 흔함. 해결: (1) 특징 확장 (2) 커널 (3) 마진 기반 학습.</p>
          <h3>특징 확장 (Feature Engineering)</h3>
          <ul>
            <li><strong>Conjunctions</strong>: 단어 동시 발생 $(w_i \land w_j)$</li>
            <li><strong>n-gram</strong>: bigram/character n-gram</li>
            <li><strong>비선형 변환</strong>: 다항식, 임계값 특징, 빈도→TF–IDF</li>
          </ul>
        </div>
        <div>
          <h3>SVM & 마진 최적화(개념)</h3>
          <div class="formula">최대 마진: \(\min_{w,b} \tfrac{1}{2}\lVert w\rVert^2 + C\sum_i \xi_i\) s.t. \(y_i(w\cdot x_i + b) \ge 1-\xi_i,\; \xi_i\ge 0\)</div>
          <p class="muted">힌지 손실 $\max(0,1 - y(w\cdot x))$ 관점으로도 표현 가능</p>
          <h3>커널 트릭(요지)</h3>
          <p>내적 $\langle \phi(x),\phi(x')\rangle$ 를 <em>커널</em> $k(x,x')$로 직접 계산 (RBF, Poly 등) → 고차원 특징을 명시적으로 만들지 않고 비선형 경계.</p>
        </div>
      </div>
      <div class="grid g-3">
        <div class="card">
          <h3>정규화 & 일반화</h3>
          <ul>
            <li>$L_2$ 정규화: 가중치 길이 억제 → 과적합 방지</li>
            <li>교차검증으로 C/커널 하이퍼파라미터 선택</li>
          </ul>
        </div>
        <div class="card">
          <h3>평가 확장</h3>
          <ul>
            <li>ROC–AUC (임계값 무관)</li>
            <li>다중분류: micro/macro F1, 혼동행렬</li>
          </ul>
        </div>
        <div class="card">
          <h3>실전 코드 포인트</h3>
          <ul>
            <li>스파스 BoW + Linear SVM은 강력하고 빠름</li>
            <li>n-gram 상한, min_df, 정규화 설정 로그로 남기기</li>
            <li>한글: 형태소/서브워드(BPE) 고려</li>
          </ul>
        </div>
      </div>

      <div class="card" style="margin-top:8px">
        <h3>실습 스니펫: Linear SVM vs RBF SVM</h3>
        <p class="muted">scikit-learn + TfidfVectorizer로 같은 데이터에서 선형/비선형 경계 비교</p>
        <pre><code class="language-python">from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC, SVC
from sklearn.metrics import classification_report

# 1) 데이터 (소규모 카테고리만 사용하면 더 빠름)
cats = ['rec.sport.baseball','sci.space','talk.politics.mideast']
X, y = fetch_20newsgroups(subset='all', categories=cats, remove=('headers','footers','quotes'),
                          return_X_y=True)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)

# 2) Linear SVM (hinge loss, large-margin)
lin = make_pipeline(TfidfVectorizer(ngram_range=(1,2), min_df=3), LinearSVC(C=1.0))
lin.fit(Xtr, ytr)
print('[LinearSVC]\n', classification_report(yte, lin.predict(Xte), digits=3))

# 3) RBF SVM (커널, 비선형 경계)
rbf = make_pipeline(TfidfVectorizer(ngram_range=(1,2), min_df=3), SVC(kernel='rbf', C=2.0, gamma='scale'))
rbf.fit(Xtr, ytr)
print('\n[RBF SVC]\n', classification_report(yte, rbf.predict(Xte), digits=3))

# 팁: C↑ → 마진↓(과적합 위험); gamma↑ → 결정경계 더 곡선화</code></pre>
      </div>

      <div class="card">
        <h3>한글 토크나이징 팁 (BPE/형태소)</h3>
        <div class="grid g-2">
          <div>
            <h4 style="margin-top:0">서브워드 BPE/SentencePiece</h4>
            <ul>
              <li>장점: OOV 견고, 띄어쓰기 오류/신조어 대응</li>
              <li>단점: 형태소 경계가 의미와 정확히 일치하지 않을 수 있음</li>
            </ul>
            <pre><code class="language-bash"># SentencePiece 학습 (unigram/BPE)
spm_train --input=corpus_ko.txt --model_prefix=spko --vocab_size=16000 \
          --model_type=bpe --character_coverage=0.9995 --pad_id=0 --unk_id=1
</code></pre>
            <pre><code class="language-python">import sentencepiece as spm
sp = spm.SentencePieceProcessor(model_file='spko.model')
sp.encode('한국어 형태소와 BPE를 같이 써볼까요?', out_type=str)
</code></pre>
          </div>
          <div>
            <h4 style="margin-top:0">형태소 분석 (Mecab/Khaiii/Kiwi 등)</h4>
            <ul>
              <li>장점: 품사/어간 분리로 짧은 데이터에서 일반화 도움</li>
              <li>단점: 도메인 적응 필요, 설치/사전 의존</li>
            </ul>
            <pre><code class="language-python"># KoNLPy 예시 (Mecab이 있다면)
from konlpy.tag import Mecab
mecab = Mecab()
mecab.pos('한국어 형태소와 BPE를 같이 써볼까요?')
# [('한국어','NNP'), ('형태소','NNG'), ('와','JC'), ('BPE','SL'), ('를','JKO'), ('같이','MAG'), ...]
</code></pre>
            <p class="muted">실무 팁: <em>형태소 BoW + Linear SVM</em> vs <em>BPE 서브워드 + 비선형/신경모델</em> 각각의 강점을 비교.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Ch.4 -->
    <section id="ch4" class="card">
      <div class="kicker">Chapter 4</div>
      <h2>Sequence Labeling (HMMs & CRFs)</h2>
      <div class="grid g-2">
        <div>
          <h3>문제 설정</h3>
          <p>입력 시퀀스 $x_{1:n}$에 대해 레이블 시퀀스 $y_{1:n}$을 예측 (예: POS 태깅, BIO NER).</p>
          <h3>HMM (Generative)</h3>
          <div class="formula">$p(y_{1:n}, x_{1:n}) = p(y_1)\,p(x_1\mid y_1)\prod_{t=2}^n p(y_t\mid y_{t-1})\,p(x_t\mid y_t)$</div>
          <ul>
            <li><strong>학습</strong>: 최대우도 (EM/Baum–Welch), 지도학습이면 카운트 정규화</li>
            <li><strong>추론</strong>: Viterbi(최적 경로), Forward–Backward(주변확률)</li>
          </ul>
        </div>
        <div>
          <h3>Linear-Chain CRF (Discriminative)</h3>
          <div class="formula">$p(y\mid x)=\dfrac{1}{Z(x)}\exp\Big(\sum_{t=1}^n \theta\cdot f(y_{t-1},y_t,x,t)\Big)$</div>
          <ul>
            <li><strong>특징</strong>: 윈도우 n-gram, 대소문자/어휘형태, 사전·Gazetteer</li>
            <li><strong>학습</strong>: 정규화된 로그우도 최대화 (L-BFGS/SGD)</li>
            <li><strong>디코딩</strong>: Viterbi with transition/feature scores</li>
          </ul>
        </div>
      </div>
      <div class="grid g-3">
        <div class="card">
          <h3>알고리즘 요약</h3>
          <div class="formula">Viterbi: $\delta_t(j)= \max_i\, \delta_{t-1}(i)+\log a_{ij}+\log b_j(x_t)$</div>
          <div class="formula">Forward: $\alpha_t(j)=\sum_i \alpha_{t-1}(i)\, a_{ij}\, b_j(x_t)$</div>
        </div>
        <div class="card">
          <h3>평가 & 오류분석</h3>
          <ul>
            <li>토큰/시퀀스 정확도, 스팬 기반 F1(BIO 스팬 일치)</li>
            <li>Confusion by tag, OOV 성능 분리 보고</li>
          </ul>
        </div>
        <div class="card">
          <h3>실전 팁</h3>
          <ul>
            <li>특징 희소성 → $L_1/L_2$ 정규화, rare feature cutoff</li>
            <li>한국어 NER: 서브워드/BPE와 문자 n-gram 혼합 효과적</li>
            <li>스팬 정답 생성 시 <code>B-</code>/<code>I-</code>/<code>O</code> 규칙 깨짐(Illegal transitions) 방지</li>
          </ul>
        </div>
      </div>
      <div class="card">
        <h3>코드 스케치 (sklearn-crfsuite)</h3>
        <pre><code class="language-python">import sklearn_crfsuite
from sklearn_crfsuite import metrics

# X_seq: token-level feature dicts, y_seq: tag sequences
crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1,
                           max_iterations=200, all_possible_transitions=True)
crf.fit(X_train, y_train)
y_pred = crf.predict(X_test)
print(metrics.flat_classification_report(y_test, y_pred))</code></pre>
        <p class="muted">특징 예: token, lower, shape, is_title, suffix/prefix, gazetteer hit, 주변 토큰 창(window)</p>
      </div>
    </section>

    <!-- Quick Reference -->
    <section id="ref" class="card">
      <h2>Quick Reference (암기 포인트)</h2>
      <div class="grid g-3">
        <div>
          <h3>공식</h3>
          <div class="formula">$\psi(x,y)=\theta\cdot f(x,y)$</div>
          <div class="formula">$p(y\mid x)=\dfrac{e^{\theta\cdot f(x,y)}}{\sum_{y'} e^{\theta\cdot f(x,y')}}$</div>
          <div class="formula">$F_1=\dfrac{2PR}{P+R}$</div>
          <div class="formula">$\text{hinge}(y,z)=\max(0,1-yz)$</div>
        </div>
        <div>
          <h3>개념 스냅</h3>
          <ul>
            <li>생성적: $p(x,y)$ vs 판별적: $p(y\mid x)$</li>
            <li>선형분리 불가 ⟶ 특징 확장/커널</li>
            <li>정규화는 일반화오차를 줄이기 위함</li>
          </ul>
        </div>
        <div>
          <h3>체크리스트</h3>
          <ul>
            <li>Train/Test 토크나이저 일치</li>
            <li>임계값·클래스 불균형 고려</li>
            <li>하이퍼파라미터와 전처리 기록</li>
          </ul>
        </div>
      </div>
      <p class="muted">Tip: <button onclick="toggleAll(true)">모든 요약 펼치기</button> / <button onclick="toggleAll(false)">모두 접기</button></p>
    </section>

    <footer class="muted">© 2025 NLP Midterm Cheat Sheet · HTML created for crisp printing (Korean & math safe).</footer>
  </div>
</body>
</html>
